{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nivapy3 as nivapy\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import toc_trends_analysis as toc_trends\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "from IPython.display import Image\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "pandas2ri.activate()\n",
    "bcp = importr('bcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC Thematic Report - February 2019 (Part 6a: analysis of annual trends; strict selection criteria)\n",
    "\n",
    "**Added 24.06.2019:** This notebook implements the original workflow (prior to the 2019 Task Force meeting) using **strict selection criteria** for which sites to include. Following discussion at the TF meeting, I have also created a new notebook (\"Part 6b\"), which applices the same workflow with less strict cirteria (i.e. a broader range of sites).\n",
    "\n",
    "Øyvind's outline for the Thematic Trends Report includes the following (see e-mail received 28/05/2019 at 12:06 for details):\n",
    "\n",
    "> My wish list for the presentation is as follows (most important first).\n",
    "> 1. MK and Sen-slope statistics (extended selection, 1990-2016)\n",
    "> 2. A comparison of trends in annual means and annual minimums for pH, ANC, alkalinity. Are they exactly the same? Here we need to use certain minimum criteria for sampling frequency (maybe minimum 4 per year for lakes and 12 per year for streams).\n",
    "> 3. A plot and table suitable for presenting the results of point 1 \n",
    "> 4. Plots/ tables that are suitable for presenting the results (1-3).\n",
    "> 5. More BCP tests for breaking points applying a less strict criteria for sampling frequency. \n",
    "\n",
    "In terms of the work done so far, [notebook 4](https://nbviewer.jupyter.org/github/JamesSample/icpw/blob/master/toc_report_feb_2019_part4_wge_plots.ipynb) produced \"spaghetti\"/scatter plots to provide an overall impression of the data, while [notebook 5](https://nbviewer.jupyter.org/github/JamesSample/icpw/blob/master/toc_report_feb_2019_part5_hi_freq.ipynb) focused on sites with high frequency data (~ weekly or better) and tested two different statistical algorithms for identifying trends and breakpoints. This notebook deals more explicitly with the points from Øyvind above. The workflow is broadly as follows:\n",
    "\n",
    " 1. Identify sites with suitable sampling records within the period from 1990 to 2016\n",
    " \n",
    " 2. Calculate annual **medians** (better than means, as requested by Øyvind?) for TOC, EH, ESO4X, ENO3, ECaX_EMgX and ANC at each site. Also calculate annual **minima** for the same parameters (to represent \"severe of episodes\")\n",
    " \n",
    " 3. Generate plots for each site showing the Sen's slope for each of series listed above. Also apply the M-K test (which usually agrees with Sen's slope, but is worth having nonetheless)\n",
    " \n",
    " 4. Apply the `bcp` algorithm from [notebook 5](https://nbviewer.jupyter.org/github/JamesSample/icpw/blob/master/toc_report_feb_2019_part5_hi_freq.ipynb) to the annual series for each site, attempting to identify breakpoints\n",
    " \n",
    " 5. Regionalise/aggregate the results from 3 and 4 above so they can be presented concisely. When I get chance I'll also run *regional* M-K and/or Sen's slope tests, but for now the simplest (and perhaps clearest?) approach will be to explore e.g. the distribution of (significant) Sen's slopes per region. I will also plot the distribution of change point years per region to see if clear patterns emerge\n",
    " \n",
    "Further refinements will be required, but this seems like a reasonable starting point given Øyvind's wish-list and current time constraints.\n",
    "\n",
    "## 1. Select sites\n",
    "\n",
    "The first step is to identify sites with sufficient sampling. Following discussion with Øyvind (see e-mails sent/received 28/05/2019), I will use the following criteria as a starting point:\n",
    "\n",
    " * **For lakes**. Aggregate to **seasonal** frequency and require that fewer than 25% of the seasonal values within the period from 1995 to 2011 are missing\n",
    " * **For rivers**. Aggregate to **monthly** frequency and require that fewer than 25% of the monthly values within the period from 1995 to 2011 are missing\n",
    " \n",
    "The original idea was that lakes should have at least one sample *per season* in order for annual means/medians to be robust, while rivers need at least one sample *per month*. We are also primarily interested in long time series - preferably spanning the whole period from 1990 to 2016. However, these criteria are rather restrictive: only a few sites have genuinely continuous records from 1990 to 2016, at either seasonal or monthly frequency. We have therefore relaxed the requirements a little, by allowing some missing data before 1995 or after 2011, and requiring the series between these years to be at least 75% complete.\n",
    "\n",
    "**Note:** The revised set of criteria are still quite restrictive - time series from some regions (e.g. Atlantic Canada and the Appalachains) are removed completely due to data gaps. We should perhaps make further adjustments here to get the right balance between robustness and inclusivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stations\n",
    "stn_path = r'../../../all_icpw_sites_may_2019.xlsx'\n",
    "stn_df = pd.read_excel(stn_path, sheet_name='all_icpw_stns')\n",
    "stn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nivapy.spatial.quickmap(stn_df,\n",
    "                        cluster=True,\n",
    "                        popup='station_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply different selection criteria for rivers versus lakes, so the next step is to extract this information from RESA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lake or river from RESA\n",
    "sql = (\"SELECT station_id, lake_or_river \"\n",
    "       \"FROM resa2.stations \"\n",
    "       \"WHERE station_id IN %s\" % str(tuple(stn_df['station_id'].values)))\n",
    "lr_df = pd.read_sql(sql, eng)\n",
    "stn_df = pd.merge(stn_df, lr_df, how='left', on='station_id')\n",
    "stn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved chem data\n",
    "wc_csv = r'../../../Thematic_Trends_Report_2019/working_chem.csv'\n",
    "wc_df = pd.read_csv(wc_csv, encoding='utf-8')\n",
    "wc_df['sample_date'] = pd.to_datetime(wc_df['sample_date'])\n",
    "wc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt to long format\n",
    "df = wc_df.copy()\n",
    "del df['station_code'], df['station_name'], df['depth1'], df['depth2']\n",
    "df = pd.melt(df, id_vars=['station_id', 'sample_date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a dataframe showing which series at which sites meet the criteria defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for percent null values in the interval 1995 to 2011\n",
    "pct_null_thresh = 25\n",
    "\n",
    "# Dict for results\n",
    "inc_dict = {'station_id':[],\n",
    "            'variable':[],\n",
    "            'include':[],\n",
    "           }\n",
    "\n",
    "# Loop over time series\n",
    "for stn_id in df['station_id'].unique():\n",
    "    # Get river or lake\n",
    "    wb_type = stn_df.query('station_id == @stn_id')['lake_or_river'].values[0]\n",
    "        \n",
    "    # Loop over variables\n",
    "    for par in df['variable'].unique():    \n",
    "        # Get data\n",
    "        df2 = df.query(\"(station_id == @stn_id) and (variable == @par)\")\n",
    "        df2.set_index('sample_date', inplace=True)\n",
    "        del df2['station_id'], df2['variable']\n",
    "        \n",
    "        # Process based on wb type\n",
    "        if wb_type == 'R':\n",
    "            # Monthly dates from 1990 - 2016\n",
    "            all_dates = pd.date_range('1990-01-01', '2016-12-31', freq='M')\n",
    "            dates_df = pd.DataFrame(index=all_dates)\n",
    "\n",
    "            # Need at least 1 sample per month\n",
    "            df2 = df2.resample('M').median()\n",
    "            df2 = dates_df.join(df2)\n",
    "            \n",
    "            if pd.isna(df2['value']).all().all():\n",
    "                # Not suitable\n",
    "                inc_dict['station_id'].append(stn_id)\n",
    "                inc_dict['variable'].append(par)\n",
    "                inc_dict['include'].append('no') \n",
    "                \n",
    "            else:\n",
    "                # Need complete record between 1995 and 2011\n",
    "                df3 = df2.truncate(before='1995-01-01', after='2011-12-31')\n",
    "                pct_null = 100*pd.isna(df3['value']).sum()/len(df3)\n",
    "                #if (pd.isna(df3['value']).sum() > 0):\n",
    "                if pct_null > pct_null_thresh:\n",
    "                    # Not suitable\n",
    "                    inc_dict['station_id'].append(stn_id)\n",
    "                    inc_dict['variable'].append(par)\n",
    "                    inc_dict['include'].append('no') \n",
    "\n",
    "                else:\n",
    "                    # Include\n",
    "                    inc_dict['station_id'].append(stn_id)\n",
    "                    inc_dict['variable'].append(par)\n",
    "                    inc_dict['include'].append('yes')   \n",
    "                    \n",
    "        if wb_type == 'L':\n",
    "            # Monthly dates from 1990 - 2016\n",
    "            all_dates = pd.date_range('1990-01-01', '2016-12-31', freq='Q-FEB')\n",
    "            dates_df = pd.DataFrame(index=all_dates)\n",
    "            \n",
    "            # Need at least 1 sample per season\n",
    "            df2 = df2.resample('Q-FEB').median()\n",
    "            df2 = dates_df.join(df2)\n",
    "            \n",
    "            if pd.isna(df2['value']).all().all():\n",
    "                # Not suitable\n",
    "                inc_dict['station_id'].append(stn_id)\n",
    "                inc_dict['variable'].append(par)\n",
    "                inc_dict['include'].append('no') \n",
    "            \n",
    "            else:\n",
    "                # Need complete record between 1995 and 2011\n",
    "                df3 = df2.truncate(before='1995-01-01', after='2011-12-31')\n",
    "                pct_null = 100*pd.isna(df3['value']).sum()/len(df3)\n",
    "                #if (pd.isna(df3['value']).sum() > 0):\n",
    "                if pct_null > pct_null_thresh:\n",
    "                    # Not suitable\n",
    "                    inc_dict['station_id'].append(stn_id)\n",
    "                    inc_dict['variable'].append(par)\n",
    "                    inc_dict['include'].append('no') \n",
    "\n",
    "                else:\n",
    "                    # Include\n",
    "                    inc_dict['station_id'].append(stn_id)\n",
    "                    inc_dict['variable'].append(par)\n",
    "                    inc_dict['include'].append('yes')\n",
    "\n",
    "# Build df\n",
    "inc_df = pd.DataFrame(inc_dict)\n",
    "\n",
    "print('The number of stations with at least some time series meeting the '\n",
    "      'specified criteria is', len(inc_df.query('include == \"yes\"')['station_id'].unique()))\n",
    "\n",
    "inc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**231 out of 555** stations have at least one time series (i.e. one parameter) that meets the specified requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of selected stations\n",
    "sel_stn_list = list(inc_df.query('include == \"yes\"')['station_id'].unique())\n",
    "sel_stns = stn_df.query('station_id in @sel_stn_list')\n",
    "nivapy.spatial.quickmap(sel_stns,\n",
    "                        cluster=True,\n",
    "                        popup='station_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save station list\n",
    "sel_stns.to_csv(r'../../../Thematic_Trends_Report_2019/results/selected_stations.csv',\n",
    "                index=False,\n",
    "                encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. M-K and Sen's slope for selected sites\n",
    "\n",
    "**Updated 08/11/2019.**\n",
    "\n",
    "The original code here considered **medians** ad **minima**. However, in an e-mail received 08.11.2019 at 14.13, Øyvind asked for comparisions of medians and **maxima**, *just for the 231 stations*. This notebook has been updated to reflect this, but note the notebook 6b (for the \"relaxed criteria\") still compared medians and **minima**.\n",
    "\n",
    "The code below produces a \"grid plot\" showing the Sen's slope trend estimate for each data series at each site (only data series meeting the selection criteria are included). Each row corresponds to a parameter, and the first column is based on  annual **medians**; the second column uses annual **maxima**.\n",
    "\n",
    "M-K and Sen's slope results for all series are also saved to a CSV file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Output folder\n",
    "out_fold = r'../../../Thematic_Trends_Report_2019/results'\n",
    "    \n",
    "# Dicts for results\n",
    "res_dict = {'station_id':[],\n",
    "            'variable':[],\n",
    "            'metric':[],\n",
    "            'mk_p_val':[],\n",
    "            'mk_trend':[],\n",
    "            'sen_slp':[],\n",
    "            'sen_incpt':[],\n",
    "            'sen_trend':[],\n",
    "           }\n",
    "\n",
    "series_dict = {}\n",
    "\n",
    "for stn_id in inc_df['station_id'].unique():\n",
    "    # Determine whether to process this site\n",
    "    inc_site_df = inc_df.query(\"(station_id == @stn_id) and (include == 'yes')\")\n",
    "    \n",
    "    if len(inc_site_df) > 0:    \n",
    "        # Setup plot\n",
    "        fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(15, 20))\n",
    "\n",
    "        # Loop over variables\n",
    "        for row_idx, par in enumerate(inc_df['variable'].unique()):\n",
    "            # Determine whether to plot series\n",
    "            inc = inc_df.query(\"(station_id == @stn_id) and (variable == @par)\")['include'].values[0]\n",
    "\n",
    "            if inc == 'no':\n",
    "                # Plot \"omitted\" text\n",
    "                axes[row_idx, 0].text(0.5, 0.5, \n",
    "                                      'Omitted due to lack of data',\n",
    "                                      verticalalignment='center', \n",
    "                                      horizontalalignment='center',\n",
    "                                      transform=axes[row_idx, 0].transAxes,\n",
    "                                      fontsize=18)\n",
    "\n",
    "                axes[row_idx, 1].text(0.5, 0.5, \n",
    "                                      'Omitted due to lack of data',\n",
    "                                      verticalalignment='center', \n",
    "                                      horizontalalignment='center',\n",
    "                                      transform=axes[row_idx, 1].transAxes,\n",
    "                                      fontsize=18)\n",
    "                \n",
    "            else:\n",
    "                # Get data\n",
    "                df2 = df.query(\"(station_id == @stn_id) and (variable == @par)\")\n",
    "                df2.set_index('sample_date', inplace=True)\n",
    "                del df2['station_id'], df2['variable']\n",
    "                series_dict[(stn_id, par)] = df2                \n",
    "                \n",
    "                # Resample to annual medians and maxes\n",
    "                for col_idx, stat in enumerate(['annual median', 'annual maximum']):\n",
    "                    if stat == 'annual median':\n",
    "                        df_stat = df2.resample('A').median()\n",
    "                    else:\n",
    "                        df_stat = df2.resample('A').max()\n",
    "                \n",
    "                    df_stat.index = df_stat.index.year\n",
    "                    \n",
    "                    # MK test\n",
    "                    mk_df = nivapy.stats.mk_test(df_stat, 'value')\n",
    "                \n",
    "                    # Sen's slope\n",
    "                    res_df, sen_df = nivapy.stats.sens_slope(df_stat, \n",
    "                                                             value_col='value',\n",
    "                                                             index_col=df_stat.index)\n",
    "                \n",
    "                    # Add results to dict\n",
    "                    res_dict['station_id'].append(stn_id)\n",
    "                    res_dict['variable'].append(par)\n",
    "                    res_dict['metric'].append(stat)\n",
    "                    res_dict['mk_p_val'].append(mk_df.loc['p'].value)\n",
    "                    res_dict['mk_trend'].append(mk_df.loc['trend'].value)\n",
    "                    \n",
    "                    sslp = res_df.loc['sslp'].value\n",
    "                    sincpt = res_df.loc['icpt'].value\n",
    "                    res_dict['sen_slp'].append(sslp)\n",
    "                    res_dict['sen_incpt'].append(sincpt)\n",
    "                    res_dict['sen_trend'].append(res_df.loc['trend'].value)\n",
    "                                        \n",
    "                    # Plot\n",
    "                    axes[row_idx, col_idx].plot(sen_df.index, sen_df['value'].values, 'bo-')\n",
    "                    axes[row_idx, col_idx].plot(sen_df.index, \n",
    "                                                sen_df.index*sslp + sincpt, 'k-')                \n",
    "                \n",
    "            if par == 'TOC':\n",
    "                unit = 'mg-C/l'\n",
    "            else:\n",
    "                unit = 'µEq/l'\n",
    "                \n",
    "            axes[row_idx, 0].set_title('Annual median %s (%s)' % (par, unit))\n",
    "            axes[row_idx, 1].set_title('Annual maximum %s (%s)' % (par, unit))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        out_png = os.path.join(out_fold, 'trends_plots_1990-2016/%s_trends_1990-2016.png' % stn_id)\n",
    "        plt.savefig(out_png, dpi=200)\n",
    "        plt.close()        \n",
    "\n",
    "# Combine results\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "out_csv = os.path.join(out_fold, 'trends_summary_1990-2016.csv')\n",
    "res_df.to_csv(out_csv, index=False, encoding='utf-8')\n",
    "\n",
    "# Save series\n",
    "out_pkl = os.path.join(out_fold, 'series.pkl')\n",
    "with open(out_pkl, 'wb') as handle:\n",
    "    pickle.dump(series_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of trends in annual means and annual maxima for EH, ANC, alkalinity\n",
    "\n",
    "**Updated 08/11/2019.**\n",
    "\n",
    "The original code here considered **medians** ad **minima**. However, in an e-mail received 08.11.2019 at 14.13, Øyvind asked for comparisions of medians and **maxima**, *just for the 231 stations*. This notebook has been updated to reflect this, but note the notebook 6b (for the \"relaxed criteria\") still compared medians and **minima**.\n",
    "\n",
    "Øyvind would like to know whether the trends in medians and maxima are consistent. One way to do this is to plot the Sen's slopes derived from medians against those based on maxima, split by parameter. Øyvind is primarily interested in EH, ANC, alkalinity, so I'll just focus on those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cols of interest\n",
    "cols = ['station_id', 'variable', 'metric', 'sen_slp']\n",
    "\n",
    "# Setup plot\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,15))\n",
    "\n",
    "# Loop over pars\n",
    "for idx, par in enumerate(['EH', 'ANC', 'ALK-E']):\n",
    "    # Get data\n",
    "    cols = ['station_id', 'variable', 'metric', 'sen_slp']\n",
    "    comp_df = res_df.query(\"variable == @par\")[cols].copy()\n",
    "    del comp_df['variable']\n",
    "    comp_df.set_index(['station_id', 'metric'], inplace=True)\n",
    "    \n",
    "    # Convert to 'wide' format\n",
    "    comp_df = comp_df.unstack('metric')\n",
    "    comp_df.columns = comp_df.columns.get_level_values('metric')\n",
    "\n",
    "    # Plot\n",
    "    axes[idx].plot(comp_df['annual median'], comp_df['annual maximum'], 'bo', label=\"Sen's slopes\")\n",
    "    axes[idx].plot(comp_df['annual median'], comp_df['annual median'], 'k-', label='1:1 line')\n",
    "    axes[idx].set_xlabel('Slope for annual medians ($yr^{-1}$)', fontsize=14)\n",
    "    axes[idx].set_ylabel('Slope for annual maxima ($yr^{-1}$)', fontsize=14)\n",
    "    axes[idx].legend(loc='best', fontsize=14)\n",
    "    axes[idx].set_title('%s (µEq/l)' % par)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'trend_maxima_vs_medians.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another visualisation option is to create **heatmaps** showing whether trends in annual minima are classified as being \"significant\" in the same way as trends in annual medians. The labels on the plots below show the percentage of the total sites for each variable in each of 9 classes. High proportions along the diagonal indicate that results based on minima and medians are giving essentially the same overall picture. The overall direction of the trends is also very clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cols of interest\n",
    "cols = ['station_id', 'variable', 'metric', 'sen_trend']\n",
    "\n",
    "# Setup plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n",
    "\n",
    "# Loop over pars\n",
    "for idx, par in enumerate(['EH', 'ANC', 'ALK-E']):\n",
    "    # Get data\n",
    "    cols = ['station_id', 'variable', 'metric', 'sen_trend']\n",
    "    comp_df = res_df.query(\"variable == @par\")[cols].copy()\n",
    "    del comp_df['variable']\n",
    "    comp_df.set_index(['station_id', 'metric'], inplace=True)\n",
    "    \n",
    "    # Convert to 'wide' format\n",
    "    comp_df = comp_df.unstack('metric')\n",
    "    comp_df.columns = comp_df.columns.get_level_values('metric')\n",
    "    comp_df.columns = [i.replace(' ', '_') for i in comp_df.columns]\n",
    "    \n",
    "    # Get counts for combos\n",
    "    opt_list = ['decreasing', 'no trend', 'increasing']\n",
    "    hmap = np.zeros(shape=(3,3))\n",
    "    \n",
    "    # Map options to array indices (med is x; min is y)\n",
    "    med_map = {'decreasing':0,\n",
    "               'no trend':1,\n",
    "               'increasing':2,\n",
    "              }\n",
    "    \n",
    "    min_map = {'decreasing':2,\n",
    "               'no trend':1,\n",
    "               'increasing':0,\n",
    "              }\n",
    "    \n",
    "    # Assign counts to array\n",
    "    for pair in itertools.product(opt_list, repeat=2):\n",
    "        ann_med, ann_max = pair\n",
    "        cnt = len(comp_df.query(\"(annual_median == @ann_med) and (annual_maximum == @ann_max)\"))\n",
    "        pct = 100*cnt/len(comp_df)\n",
    "        hmap[min_map[ann_max], med_map[ann_med]] = pct\n",
    "\n",
    "    # Plot\n",
    "    g = sn.heatmap(hmap, \n",
    "                   ax=axes[idx], \n",
    "                   square=True, \n",
    "                   annot=True,\n",
    "                   xticklabels=opt_list,\n",
    "                   yticklabels=opt_list[::-1],\n",
    "                   cmap='coolwarm',\n",
    "                  )\n",
    "    g.set_yticklabels(g.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    axes[idx].set_xlabel('Annual medians', fontsize=16)\n",
    "    axes[idx].set_ylabel('Annual maxima', fontsize=16)\n",
    "    axes[idx].set_title('%s (%%)' % par)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'trend_maxima_vs_medians_heatmap.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarising by region\n",
    "\n",
    "To present results concisely, we need to aggregate output from the analysis above. A simple way to get an overall picture of the dataset is to create box plots, violin plots and histograms illustrating the **range of *significant* Sen's slopes** for each parameter in each region (or country or continent). I will **focus on annual medians** here since, based on the output above, results for minima should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "reg_df = res_df.query(\"(metric == 'annual median') and \"\n",
    "                      \"(sen_trend in ('increasing', 'decreasing'))\")\n",
    "\n",
    "# Join regions\n",
    "reg_df = pd.merge(reg_df, stn_df[['station_id', 'continent', 'country', 'region']],\n",
    "                  how='left', on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots by region\n",
    "g = sn.catplot(data=reg_df,\n",
    "               row='variable',\n",
    "               x='region',\n",
    "               y='sen_slp',\n",
    "               kind='box',\n",
    "               order=['NoNord', 'SoNord', 'Baltic', 'UK-IE-NL', \n",
    "                      'WCE', 'ECE', 'Alps', 'AtlCan', 'QuMaVt',\n",
    "                      'AdsCsk', 'Apps', 'BRM', 'Ont'],\n",
    "               sharex=False,\n",
    "               sharey=False,\n",
    "               aspect=3\n",
    "              )\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'slope_box_plots_by_region.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots by continent\n",
    "g = sn.catplot(data=reg_df,\n",
    "               col='variable',\n",
    "               col_wrap=4,\n",
    "               x='continent',\n",
    "               y='sen_slp',\n",
    "               kind='violin',\n",
    "               sharex=False,\n",
    "               sharey=False,\n",
    "               aspect=1\n",
    "              )\n",
    "\n",
    "g.map(plt.axhline, y=0, lw=2, ls='--', c='k')\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'slope_violin_plots_by_continent.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by continent\n",
    "g = sn.FacetGrid(reg_df,\n",
    "                 row=\"variable\",\n",
    "                 col='continent',\n",
    "                 aspect=2,\n",
    "                 sharex='row',\n",
    "                 sharey=False)\n",
    "g.map(sn.distplot, 'sen_slp', hist=False, rug=True)\n",
    "g.map(plt.axvline, x=0, lw=2, ls='--', c='k')\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'slope_kde_plots_by_continent.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that all the plots above are different ways of illustrating the distributions of *significant* Sen's slopes. I think there are some interesting patterns here - especially comparing Europe with North America.\n",
    "\n",
    "## 5. Change points\n",
    "\n",
    "The code below uses the BCP R package and is modified from [notebook 5](https://nbviewer.jupyter.org/github/JamesSample/icpw/blob/master/toc_report_feb_2019_part5_hi_freq.ipynb). \n",
    "\n",
    "For each of the selected time series at each site, I will run a change point analysis (without regression - see [here](https://nbviewer.jupyter.org/github/JamesSample/icpw/blob/master/toc_report_feb_2019_part5_hi_freq.ipynb#3.-BCP-package-in-R) for details) and produce a single plot showing all the data for each site. For each variable, I will also record years when the probabiltiy of change is (i) greater than 50% and (ii) greater than 75%. This will make it possible to look for consistent change points across regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listvector_to_dict(lv):\n",
    "    \"\"\" Convert R ListVector to a Python dict.\n",
    "    \"\"\"\n",
    "    return dict(zip(lv.names, map(list,list(lv))))\n",
    "\n",
    "def plot_bcp_change_pts(stn_id, reg=True, p0=0.2):\n",
    "    \"\"\" Run BCP analysis to detect change-points.\n",
    "    \"\"\"\n",
    "    res_dict = {'station_id':[],\n",
    "                'variable':[],\n",
    "                'prob':[],\n",
    "                'year':[],\n",
    "               }\n",
    "    \n",
    "    # Setup plot\n",
    "    fig, axes = plt.subplots(nrows=7, ncols=2, \n",
    "                             sharex=False,\n",
    "                             sharey=False,\n",
    "                             figsize=(15,15))\n",
    "    \n",
    "    # Loop over variables\n",
    "    for row_idx, par in enumerate(inc_df['variable'].unique()):\n",
    "        # Determine whether to plot series\n",
    "        inc = inc_df.query(\"(station_id == @stn_id) and (variable == @par)\")['include'].values[0]\n",
    "\n",
    "        if inc == 'no':\n",
    "            # Plot \"omitted\" text\n",
    "            axes[row_idx, 0].text(0.5, 0.5, \n",
    "                                  'Omitted due to lack of data',\n",
    "                                  verticalalignment='center', \n",
    "                                  horizontalalignment='center',\n",
    "                                  transform=axes[row_idx, 0].transAxes,\n",
    "                                  fontsize=18)\n",
    "            axes[row_idx, 0].set_ylabel(par)\n",
    "\n",
    "            axes[row_idx, 1].text(0.5, 0.5, \n",
    "                                  'Omitted due to lack of data',\n",
    "                                  verticalalignment='center', \n",
    "                                  horizontalalignment='center',\n",
    "                                  transform=axes[row_idx, 1].transAxes,\n",
    "                                  fontsize=18)\n",
    "            axes[row_idx, 1].set_ylabel('Probability')\n",
    "            \n",
    "        else:\n",
    "            # Get data\n",
    "            df2 = df.query(\"(station_id == @stn_id) and (variable == @par)\")\n",
    "            df2.set_index('sample_date', inplace=True)\n",
    "            del df2['station_id'], df2['variable']\n",
    "    \n",
    "            # Resample\n",
    "            df2 = df2.resample('A').median().reset_index()\n",
    "            df2['year'] = df2['sample_date'].dt.year\n",
    "            del df2['sample_date']\n",
    "\n",
    "            # Interpolate years with missing data\n",
    "            index_df = pd.DataFrame({'year':range(1990, 2017)})\n",
    "            df2 = pd.merge(index_df, df2, how='left', on='year')\n",
    "            df2.interpolate(kind='linear', inplace=True)\n",
    "            df2.fillna(method='backfill', inplace=True)\n",
    "\n",
    "            # Change point analysis\n",
    "            if reg:\n",
    "                # Perform regression within each partition\n",
    "                res = bcp.bcp(df2['value'], \n",
    "                              x=df['year'], \n",
    "                              p0=p0)\n",
    "            else:\n",
    "                # Assume constant mean within each partition\n",
    "                res = bcp.bcp(df2['value'], \n",
    "                              p0=p0)            \n",
    "\n",
    "            res = pd.DataFrame({'raw':df2['value'].values,\n",
    "                                'mean':res.rx2('posterior.mean').flatten(),\n",
    "                                'prob':res.rx2('posterior.prob')},\n",
    "                               index=df2['year'].values)\n",
    "\n",
    "            # Add to results\n",
    "            gt50 = np.array(res.index[res['prob'] > 0.5])\n",
    "            for year in gt50:                        \n",
    "                res_dict['station_id'].append(stn_id)\n",
    "                res_dict['variable'].append(par)\n",
    "                res_dict['prob'].append('gt50')\n",
    "                res_dict['year'].append(year)\n",
    "            \n",
    "            gt75 = np.array(res.index[res['prob'] > 0.75])\n",
    "            for year in gt75:                        \n",
    "                res_dict['station_id'].append(stn_id)\n",
    "                res_dict['variable'].append(par)\n",
    "                res_dict['prob'].append('gt75')\n",
    "                res_dict['year'].append(year)  \n",
    "                \n",
    "            # Plot\n",
    "            res['raw'].plot(ax=axes[row_idx, 0], marker='o', linestyle=':', label='Raw')\n",
    "            res['mean'].plot(ax=axes[row_idx, 0], label='Fitted')\n",
    "            axes[row_idx, 0].set_ylabel(par)\n",
    "            axes[row_idx, 0].legend(loc='best')\n",
    "\n",
    "            res['prob'].plot(ax=axes[row_idx, 1], marker='o')\n",
    "            axes[row_idx, 1].set_ylabel('Probability')\n",
    "            axes[row_idx, 1].set_ylim((0, 1))\n",
    "            axes[row_idx, 1].axhline(0.5, c='k', linestyle='--')\n",
    "            axes[row_idx, 1].axhline(0.95, c='k', linestyle='--')\n",
    "\n",
    "        axes[0, 0].set_title('Data series')    \n",
    "        axes[0, 1].set_title('Change probability')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(out_fold, 'bcp_selected_sites/bcp_stn_%s.png' % stn_id)\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "    \n",
    "    res_df = pd.DataFrame(res_dict)\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Container for results\n",
    "df_list = []\n",
    "\n",
    "# Loop over stations\n",
    "for stn_id in inc_df['station_id'].unique():\n",
    "    # Determine whether to process this site\n",
    "    inc_site_df = inc_df.query(\"(station_id == @stn_id) and (include == 'yes')\")\n",
    "    \n",
    "    if len(inc_site_df) > 0: \n",
    "        # Run BCP\n",
    "        res_bcp = plot_bcp_change_pts(stn_id, reg=False, p0=0.1)  \n",
    "        df_list.append(res_bcp)\n",
    "        \n",
    "# Combine results\n",
    "res_bcp = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example BCP output\n",
    "Image(r'../../../Thematic_Trends_Report_2019/results/bcp_selected_sites/bcp_stn_38242.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results can now be aggregated regionally. I will produce four sets of plots:\n",
    "\n",
    " 1. KDE-smoothed histograms showing years where the probabiltiy of change is **>50%**\n",
    " \n",
    "  a. Split by region  \n",
    "  \n",
    "  b. Split by continent\n",
    "   \n",
    " 2. KDE-smoothed histograms showing years where the probabiltiy of change is **>75%**\n",
    " \n",
    "  a. Split by region\n",
    "  \n",
    "  b. Split by continent\n",
    "  \n",
    "For ease of comparison, columns represent parameters and rows are regions. Considering one column at a time therefore makes it posible to identify for similar patterns between regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join regions\n",
    "res_bcp = pd.merge(res_bcp, stn_df[['station_id', 'continent', 'country', 'region']],\n",
    "                   how='left', on='station_id')\n",
    "\n",
    "res_bcp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by region\n",
    "# Change prob > 50%\n",
    "gt50_df = res_bcp.query('prob == \"gt50\"')\n",
    "\n",
    "g = sn.FacetGrid(gt50_df,\n",
    "                 row='region',\n",
    "                 col='variable',\n",
    "                 aspect=1.5,\n",
    "                 sharex='row',\n",
    "                 sharey=False,\n",
    "                 row_order=['NoNord', 'SoNord', 'UK-IE-NL', \n",
    "                            'WCE', 'ECE', 'Alps', 'QuMaVt',\n",
    "                            'AdsCsk', 'BRM', 'Ont'])\n",
    "g.map(sn.distplot, 'year')\n",
    "g.set(xlim=(1990, 2016))\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'change_prob_kde_gt50_by_region.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by region\n",
    "# Change prob > 75%\n",
    "gt50_df = res_bcp.query('prob == \"gt75\"')\n",
    "\n",
    "g = sn.FacetGrid(gt50_df,\n",
    "                 row='region',\n",
    "                 col='variable',\n",
    "                 aspect=1.5,\n",
    "                 sharex='row',\n",
    "                 sharey=False,\n",
    "                 row_order=['NoNord', 'SoNord', 'UK-IE-NL', \n",
    "                            'WCE', 'ECE', 'Alps', 'QuMaVt',\n",
    "                            'AdsCsk', 'BRM', 'Ont'])\n",
    "g.map(sn.distplot, 'year')\n",
    "g.set(xlim=(1990, 2016))\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'change_prob_kde_gt75_by_region.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by continent\n",
    "# Change prob > 50%\n",
    "gt50_df = res_bcp.query('prob == \"gt50\"')\n",
    "\n",
    "g = sn.FacetGrid(gt50_df,\n",
    "                 row='continent',\n",
    "                 col='variable',\n",
    "                 aspect=1.5,\n",
    "                 sharex='row',\n",
    "                 sharey=False)\n",
    "g.map(sn.distplot, 'year')\n",
    "g.set(xlim=(1990, 2016))\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'change_prob_kde_gt50_by_continent.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by continent\n",
    "# Change prob > 75%\n",
    "gt50_df = res_bcp.query('prob == \"gt75\"')\n",
    "\n",
    "g = sn.FacetGrid(gt50_df,\n",
    "                 row='continent',\n",
    "                 col='variable',\n",
    "                 aspect=1.5,\n",
    "                 sharex='row',\n",
    "                 sharey=False)\n",
    "g.map(sn.distplot, 'year')\n",
    "g.set(xlim=(1990, 2016))\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'change_prob_kde_gt75_by_continent.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't had time to look at this output in detail, but there appears to be evidence for consistent regional changes in at least some variables. For example, ENO3X, ECaX_EMgX and ESO4X all show similar patterns in the SoNord, NoNord and UK+IE+NL regions (and perhaps for Europe as a whole too), with many change points occurring in the mid-1990s. For TOC, meanwhile, the most common European change points are about a decade later, in the mid-2000s.\n",
    "\n",
    "## 6. Differences between medians and minima\n",
    "\n",
    "**Added 02/06/2019.**\n",
    "\n",
    "Øyvind would like to explore trends in the absolute differences between annual median and minimum values for ANC and pH. This is motivated by some interesting patterns previously documented at Øygardsbekken (station ID 38313) - see e-mail from Øyvind received 01/06/2019 at 17.47 for details.\n",
    "\n",
    "As a sanity check, it's a good idea to first make sure I can reproduce the patterns in Øyvind's e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load series\n",
    "pkl = os.path.join(out_fold, 'series.pkl')\n",
    "with open(pkl, 'rb') as handle:\n",
    "    series_dict = pickle.load(handle)\n",
    "    \n",
    "# Get data for Øygardsbekken\n",
    "stn_id = 38313\n",
    "\n",
    "# Loop over dfs\n",
    "df_list = []\n",
    "\n",
    "for par in ['ANC', 'EH']:\n",
    "    # Get data\n",
    "    df = series_dict[(stn_id, par)]   \n",
    "    \n",
    "    # Convert EH to pH\n",
    "    if par == 'EH':\n",
    "        df['value'] = -np.log10(df['value']/1E6) \n",
    "        par = 'pH'\n",
    "        \n",
    "    # Resample to annual medians and mins\n",
    "    for col_idx, stat in enumerate(['annual median', 'annual minimum']):\n",
    "        if stat == 'annual median':\n",
    "            df_stat = df.resample('A').median()\n",
    "        else:\n",
    "            df_stat = df.resample('A').min()\n",
    "\n",
    "        df_stat['year'] = df_stat.index.year\n",
    "        df_stat['metric'] = stat\n",
    "        df_stat['variable'] = par\n",
    "        df_stat.reset_index(inplace=True, drop=True)\n",
    "            \n",
    "        df_list.append(df_stat)\n",
    "        \n",
    "ann_df = pd.concat(df_list, axis=0)        \n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "g = sn.lmplot(x='year', \n",
    "              y='value', \n",
    "              data=ann_df,\n",
    "              col='variable',\n",
    "              hue='metric',\n",
    "              sharey=False,\n",
    "              lowess=True)\n",
    "\n",
    "g.axes[0, 0].set_ylim((-125, 75))\n",
    "g.axes[0, 1].set_ylim((4, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots look the same as in Øyvind's e-mail, which is good. The difference between minimum and median ANC looks fairly steady, whereas differences for pH are becoming larger through time, apparently because median pH is increasing more rapidly than minimum pH. The next step is to calculate absolute differences between median and minimum values for all sites and then test for trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load series\n",
    "pkl = os.path.join(out_fold, 'series.pkl')\n",
    "with open(pkl, 'rb') as handle:\n",
    "    series_dict = pickle.load(handle)\n",
    "    \n",
    "# Dicts for results\n",
    "res_dict = {'station_id':[],\n",
    "            'variable':[],\n",
    "            'mk_p_val':[],\n",
    "            'mk_trend':[],\n",
    "            'sen_slp':[],\n",
    "            'sen_incpt':[],\n",
    "            'sen_trend':[],\n",
    "           }\n",
    "\n",
    "for stn_id in inc_df['station_id'].unique():\n",
    "    for par in ['ANC', 'EH']:\n",
    "        try:\n",
    "            # Get data\n",
    "            df = series_dict[(stn_id, par)]   \n",
    "\n",
    "            # Convert EH to pH\n",
    "            if par == 'EH':\n",
    "                df['value'] = -np.log10(df['value']/1E6) \n",
    "                par = 'pH'\n",
    "\n",
    "            # Resample to annual medians and mins\n",
    "            df_med = df.resample('A').median()\n",
    "            df_min = df.resample('A').min()\n",
    "            df_diff = df_med - df_min\n",
    "            df_diff.index = df_diff.index.year\n",
    "\n",
    "            # MK test\n",
    "            mk_df = nivapy.stats.mk_test(df_diff, 'value')\n",
    "\n",
    "            # Sen's slope\n",
    "            res_df, sen_df = nivapy.stats.sens_slope(df_diff, \n",
    "                                                     value_col='value',\n",
    "                                                     index_col=df_diff.index)\n",
    "\n",
    "            # Add results to dict\n",
    "            res_dict['station_id'].append(stn_id)\n",
    "            res_dict['variable'].append(par)\n",
    "            res_dict['mk_p_val'].append(mk_df.loc['p'].value)\n",
    "            res_dict['mk_trend'].append(mk_df.loc['trend'].value)\n",
    "\n",
    "            sslp = res_df.loc['sslp'].value\n",
    "            sincpt = res_df.loc['icpt'].value\n",
    "            res_dict['sen_slp'].append(sslp)\n",
    "            res_dict['sen_incpt'].append(sincpt)\n",
    "            res_dict['sen_trend'].append(res_df.loc['trend'].value)\n",
    "                                       \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "# Combine results\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "out_csv = os.path.join(out_fold, 'trends_in_differences.csv')\n",
    "res_df.to_csv(out_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further quick check, here are the results for Øygardsbekken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for Øygardsbekken\n",
    "res_df.query('station_id == 38313')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is no trend for differences in ANC, but a significant increasing trend for differences in pH.\n",
    "\n",
    "Next, join in the region data and explore patterns for all sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just significant\n",
    "df = res_df.query(\"sen_trend in ('increasing', 'decreasing')\")\n",
    "\n",
    "# Join regions\n",
    "df = pd.merge(df, stn_df[['station_id', 'continent', 'country', 'region']],\n",
    "              how='left', on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of significant slopes in differences by region\n",
    "g = sn.catplot(data=df,\n",
    "               x='region',\n",
    "               y='sen_slp',\n",
    "               row='variable',\n",
    "               kind='box', \n",
    "               order=['NoNord', 'SoNord', 'Baltic', 'UK-IE-NL', \n",
    "                      'WCE', 'ECE', 'Alps', 'AtlCan', 'QuMaVt',\n",
    "                      'AdsCsk', 'Apps', 'BRM', 'Ont'],\n",
    "               sharex=False,\n",
    "               sharey=False,\n",
    "               aspect=3,\n",
    "              ) \n",
    "\n",
    "g.map(plt.axhline, y=0, lw=2, ls='--', c='k', alpha=0.4)\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'med_min_diff_box_plots_by_region.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-smoothed plots by continent\n",
    "g = sn.FacetGrid(df,\n",
    "                 row='variable',\n",
    "                 col='continent',\n",
    "                 aspect=2,\n",
    "                 sharex='row',\n",
    "                 sharey=False)\n",
    "g.map(sn.distplot, 'sen_slp', hist=False, rug=True)\n",
    "g.map(plt.axvline, x=0, lw=2, ls='--', c='k')\n",
    "\n",
    "# Save\n",
    "out_png = os.path.join(out_fold, 'med_min_diff_kde_plots_by_continent.png')\n",
    "plt.savefig(out_png, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results vary considerably between regions and, from a quick glance, I don't think there's a clear picture here. The increasing trend in pH differences at Øygardsbekken seems typical of some (but not all) sites in the Southern Nordic region, and also of sites in the UK, Ireland and the Netherlands. All these regions likely have fairly high marine inputs, which perhaps supports Øyvind's hypothesis that these patterns are driven by sea salt episodes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
